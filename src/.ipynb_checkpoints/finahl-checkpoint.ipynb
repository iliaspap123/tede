{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import matplotlib.pyplot as plt #for plot\n",
    "import nltk\n",
    "#read_csv?\n",
    "Location = r'../twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(Location,delimiter = '\\t',names=['id','id2','tag','text'])\n",
    "# df.text\n",
    "\n",
    "Location_test = r'../twitter_data/test2017.tsv'\n",
    "df_test = pd.read_csv(Location_test,delimiter = '\\t',names=['id','id2','tag','text'])\n",
    "\n",
    "\n",
    "Location_corects = r'../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt'\n",
    "df_corects = pd.read_csv(Location_corects,delimiter = '\\t',names=['id','tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import StemmerI, RegexpStemmer, LancasterStemmer, ISRIStemmer, PorterStemmer, SnowballStemmer, RSLPStemmer\n",
    "from string import punctuation\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tweets = df.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(tweet):\n",
    "    tweet = tweet.lower() \n",
    "    tweet = BeautifulSoup(tweet,'lxml').get_text() \n",
    "    tweet = re.sub(r\"@\\w+\", \"\",tweet) # Remove twitter handle\n",
    "    tweet = re.sub(r\"\\d\", \"\",tweet)   # Remove numbers \n",
    "    tweet = re.sub(r\"_+\", \"\",tweet)   # Remove consecutive underscores\n",
    "    tweet = re.sub('www.?[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = re.sub('http?[A-Za-z0-9./]+','',tweet)\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    #tweet=tweet.strip(punctuation)\n",
    "    tokens = word_tokenize(tweet)\n",
    "    stems = [  stemmer.stem(token) for token in tokens ]\n",
    "    lemms = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stems]\n",
    "    filtered = [w for w in lemms if not w in stopwords.words('english')]\n",
    "    tweet = ' '.join(filtered)\n",
    "    return tweet\n",
    "\n",
    "clean_tweets = df.text.apply(clean_text)\n",
    "\n",
    "# print(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with loading all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from scipy.misc import imread\n",
    "\n",
    "twitter_mask = imageio.imread('./twitter_mask.png')\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "                      background_color='white',\n",
    "                      width=1800,\n",
    "                      height=1400,\n",
    "                      mask=twitter_mask).generate(clean_tweets.to_string())\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('./my_twitter_wordcloud_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_test = df_test.text.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexica(text):\n",
    "    Location_affin = r'../lexica/generic/generic.txt'\n",
    "    mydict = {}\n",
    "    with open(Location_affin) as f:\n",
    "        for line in f:\n",
    "            if len(line.split()) == 2:\n",
    "                (key,value) = line.split()\n",
    "                mydict[key] = value\n",
    "\n",
    "\n",
    "    import random\n",
    "    metadata = []\n",
    "    for tweet in text:\n",
    "        arr = []\n",
    "        tmp = []\n",
    "        tmp.append(len(tweet.split()))\n",
    "        for word in tweet.split():\n",
    "            if word in mydict:\n",
    "                arr.append(float(mydict[word]))\n",
    "        if arr:\n",
    "            tmp.append(min(arr)) \n",
    "            if min(arr) == max(arr):\n",
    "                ran = random.randint(-3,3)\n",
    "                tmp.append(ran)\n",
    "                tmp.append(((ran+min(arr))/ 2))  \n",
    "            else:\n",
    "                tmp.append(max(arr))  \n",
    "                tmp.append(float(sum(arr) / len(arr)))  \n",
    "        else:\n",
    "            ran1 = random.randint(-3,3)\n",
    "            ran2 = random.randint(-3,3)\n",
    "            tmp.append(ran1)\n",
    "            tmp.append(ran2)\n",
    "            tmp.append(((ran1+ran2) / 2))  \n",
    "\n",
    "        metadata.append(tmp)\n",
    "    return metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "bow_vectorizer = CountVectorizer( max_df=1.0,min_df=1,max_features=300, stop_words='english') \n",
    "bow_xtrain = bow_vectorizer.fit_transform(clean_tweets)  #TWEETS : a list with the actual tweets \n",
    "\n",
    "bow_xtest = bow_vectorizer.fit_transform(clean_tweets_test)  #TWEETS : a list with the actual tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow_xtrain.toarray()\n",
    "Y = df.tag\n",
    "knn.fit(X , Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn.predict(bow_xtest.toarray())\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(result, df_corects.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = lexica(df.text)\n",
    "arr_train = []\n",
    "arr_test = []\n",
    "for vect in zip(bow_xtrain.toarray(),np.array(metadata)): \n",
    "    arr_train.append(np.append(vect[0],vect[1]))\n",
    "for vect in zip(bow_xtest.toarray(),np.array(metadata)): \n",
    "    arr_test.append(np.append(vect[0],vect[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(arr_train , Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn.predict(arr_test)\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(result, df_corects.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
