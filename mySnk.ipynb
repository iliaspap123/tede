{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My First Try\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Gas by my house hit $3.39!!!! I'm going to Cha...\n",
       "1    Theo Walcott is still shit, watch Rafa and Joh...\n",
       "2    its not that I'm a GSP fan, i just hate Nick D...\n",
       "3    Iranian general says Israel's Iron Dome can't ...\n",
       "4    Tehran, Mon Amour: Obama Tried to Establish Ti...\n",
       "5    I sat through this whole movie just for Harry ...\n",
       "6    with J Davlar 11th. Main rivals are team Polan...\n",
       "7    Talking about ACT's && SAT's, deciding where I...\n",
       "8    Why is \\\"\"Happy Valentines Day\\\"\" trending? It...\n",
       "9    They may have a SuperBowl in Dallas, but Dalla...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame, read_csv\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import matplotlib.pyplot as plt #for plot\n",
    "import nltk\n",
    "\n",
    "\n",
    "read_csv?\n",
    "Location = r'../twitter_data/train2017.tsv'\n",
    "df = pd.read_csv(Location,delimiter = '\\t',nrows=10,names=['id','id2','tag','text'])\n",
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', '!', 'I', \"'m\", 'going', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':', ')'], ['Theo', 'Walcott', 'is', 'still', 'shit', ',', 'watch', 'Rafa', 'and', 'Johnny', 'deal', 'with', 'him', 'on', 'Saturday', '.'], ['its', 'not', 'that', 'I', \"'m\", 'a', 'GSP', 'fan', ',', 'i', 'just', 'hate', 'Nick', 'Diaz', '.', 'ca', \"n't\", 'wait', 'for', 'february', '.'], ['Iranian', 'general', 'says', 'Israel', \"'s\", 'Iron', 'Dome', 'ca', \"n't\", 'deal', 'with', 'their', 'missiles', '(', 'keep', 'talking', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'finding', 'out', ')'], ['Tehran', ',', 'Mon', 'Amour', ':', 'Obama', 'Tried', 'to', 'Establish', 'Ties', 'with', 'the', 'Mullahs', 'http', ':', '//t.co/TZZzrrKa', 'via', '@', 'PJMedia_com', 'No', 'Barack', 'Obama', '-', 'Vote', 'Mitt', 'Romney'], ['I', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'Harry', 'and', 'Ron', 'at', 'christmas', '.', 'ohlawd'], ['with', 'J', 'Davlar', '11th', '.', 'Main', 'rivals', 'are', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.'], ['Talking', 'about', 'ACT', \"'s\", '&', '&', 'SAT', \"'s\", ',', 'deciding', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'applying', 'to', 'colleges', 'and', 'everything', 'about', 'college', 'stresses', 'me', 'out', '.'], ['Why', 'is', '\\\\', \"''\", \"''\", 'Happy', 'Valentines', 'Day\\\\', \"''\", \"''\", 'trending', '?', 'It', \"'s\", 'on', 'the', '14th', 'of', 'February', 'not', '12th', 'of', 'June', 'smh..'], ['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', 'ai', \"n't\", 'winning', 'a', 'SuperBowl', '.', 'Not', 'with', 'that', 'quarterback', 'and', 'owner', '.', '@', 'S4NYC', '@', 'RasmussenPoll']]\n"
     ]
    }
   ],
   "source": [
    "#df['id'].plot()\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "list_token = []\n",
    "#i = 0\n",
    "for line in df.text:\n",
    "#    print(line)\n",
    "    token = word_tokenize(line)\n",
    "    list_token.append(token)\n",
    "#    print(token)\n",
    "#     i = i+1\n",
    "\n",
    "print(list_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stemming\n",
    "'''\n",
    "\n",
    "from nltk.stem import StemmerI, RegexpStemmer, LancasterStemmer, ISRIStemmer, PorterStemmer, SnowballStemmer, RSLPStemmer\n",
    "\n",
    "#stemmer = WordNetLemmatizer()\n",
    "#stemmer = LancasterStemmer()\n",
    "#stemmer = SnowballStemmer('english')\n",
    "# #stemmer = PorterStemmer()\n",
    "# from nltk.stem import  WordNetLemmatizer\n",
    "# stemmer = WordNetLemmatizer()\n",
    "# stems = [  stemmer.stem(token) for token in list_token[0] ]\n",
    "# print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas', 'by', 'my', 'house', 'hit', '$', '3.39', '!', '!', '!', '!', 'I', \"'m\", 'go', 'to', 'Chapel', 'Hill', 'on', 'Sat', '.', ':', ')']\n",
      "['Theo', 'Walcott', 'be', 'still', 'shit', ',', 'watch', 'Rafa', 'and', 'Johnny', 'deal', 'with', 'him', 'on', 'Saturday', '.']\n",
      "['it', 'not', 'that', 'I', \"'m\", 'a', 'GSP', 'fan', ',', 'i', 'just', 'hate', 'Nick', 'Diaz', '.', 'ca', \"n't\", 'wait', 'for', 'february', '.']\n",
      "['Iranian', 'general', 'say', 'Israel', \"'s\", 'Iron', 'Dome', 'ca', \"n't\", 'deal', 'with', 'their', 'missile', '(', 'keep', 'talk', 'like', 'that', 'and', 'we', 'may', 'end', 'up', 'find', 'out', ')']\n",
      "['Tehran', ',', 'Mon', 'Amour', ':', 'Obama', 'Tried', 'to', 'Establish', 'Ties', 'with', 'the', 'Mullahs', 'http', ':', '//t.co/TZZzrrKa', 'via', '@', 'PJMedia_com', 'No', 'Barack', 'Obama', '-', 'Vote', 'Mitt', 'Romney']\n",
      "['I', 'sat', 'through', 'this', 'whole', 'movie', 'just', 'for', 'Harry', 'and', 'Ron', 'at', 'christmas', '.', 'ohlawd']\n",
      "['with', 'J', 'Davlar', '11th', '.', 'Main', 'rival', 'be', 'team', 'Poland', '.', 'Hopefully', 'we', 'an', 'make', 'it', 'a', 'successful', 'end', 'to', 'a', 'tough', 'week', 'of', 'training', 'tomorrow', '.']\n",
      "['Talking', 'about', 'ACT', \"'s\", '&', '&', 'SAT', \"'s\", ',', 'decide', 'where', 'I', 'want', 'to', 'go', 'to', 'college', ',', 'apply', 'to', 'college', 'and', 'everything', 'about', 'college', 'stress', 'me', 'out', '.']\n",
      "['Why', 'be', '\\\\', \"''\", \"''\", 'Happy', 'Valentines', 'Day\\\\', \"''\", \"''\", 'trend', '?', 'It', \"'s\", 'on', 'the', '14th', 'of', 'February', 'not', '12th', 'of', 'June', 'smh..']\n",
      "['They', 'may', 'have', 'a', 'SuperBowl', 'in', 'Dallas', ',', 'but', 'Dallas', 'ai', \"n't\", 'win', 'a', 'SuperBowl', '.', 'Not', 'with', 'that', 'quarterback', 'and', 'owner', '.', '@', 'S4NYC', '@', 'RasmussenPoll']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "for line in df.text:\n",
    "    print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(line)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
